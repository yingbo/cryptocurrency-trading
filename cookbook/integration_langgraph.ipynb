{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJsVa1O4TuL"
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Open Source Observability for LangGraph\" description: \"Learn how to use Langfuse for open source observability/tracing in your LangGraph application (Python).\" category: \"Integrations\" -->\n",
        "\n",
        "# Cookbook: LangGraph Integration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlCI9KeX4Zn4"
      },
      "source": [
        "## What is LangGraph?\n",
        "\n",
        "[LangGraph](https://langchain-ai.github.io/langgraph/) is an open-source framework by the LangChain team for building complex, stateful, multi-agent applications using large language models (LLMs). LangGraph includes built-in persistence to save and resume state, which enables error recovery and human-in-the-loop workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o8L1qPcaZeC"
      },
      "source": [
        "## Goal of this Cookbook\n",
        "\n",
        "This cookbook demonstrates how [Langfuse](https://langfuse.com/docs) helps to debug, analyze, and iterate on your LangGraph application using the [LangChain integration](https://langfuse.com/docs/integrations/langchain/tracing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPTaMtxH4eHV"
      },
      "source": [
        "**By the end of this cookbook, you will be able to:**\n",
        "\n",
        "\n",
        "*   Automatically trace LangGraph application via the Langfuse integration\n",
        "*   Monitor advanced multi-agent setups\n",
        "*   Add scores (like user feedback)\n",
        "*   Manage your prompts used in LangGraph with Langfuse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sSIS88y9Ewm"
      },
      "source": [
        "## Initialize Langfuse\n",
        "\n",
        "Initialize the Langfuse client with your [API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys) from the project settings in the Langfuse UI and add them to your environment.\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"âš ï¸\" -->\n",
        "_**Note:** This notebook utilizes the Langfuse Python SDK v3. For users of Python SDK v2, please refer to our [legacy LangGraph integration guide](https://github.com/langfuse/langfuse-docs/blob/662509b3296daddcddb292f14b10a62e7c39407d/cookbook/integration_langgraph.ipynb)._\n",
        "<!-- CALLOUT_END -->\n",
        "\n",
        "<!-- CALLOUT_START type: \"info\" emoji: \"â„¹ï¸\" -->\n",
        "_**Note:** You need to run at least Python 3.11 ([GitHub Issue](https://github.com/langfuse/langfuse/issues/1926))._\n",
        "<!-- CALLOUT_END -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C85BK1vJ5yD3",
        "outputId": "92479d44-4667-44e7-ba94-ec54beea463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langfuse\n",
            "  Downloading langfuse-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting backoff>=1.10.0 (from langfuse)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.11/dist-packages (from langfuse) (0.28.1)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp<2.0.0,>=1.33.1 (from langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langfuse) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.11/dist-packages (from langfuse) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langfuse) (2.32.3)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.11/dist-packages (from langfuse) (1.17.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.93.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (1.73.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langfuse) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langfuse) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langfuse-3.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.5.1-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, ormsgpack, opentelemetry-proto, mypy-extensions, marshmallow, httpx-sse, backoff, typing-inspect, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, pydantic-settings, opentelemetry-semantic-conventions, langgraph-sdk, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, langchain_openai, opentelemetry-exporter-otlp, langgraph-prebuilt, langgraph, langfuse, langchain_community\n",
            "Successfully installed backoff-2.2.1 dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 langchain_openai-0.3.27 langfuse-3.1.2 langgraph-0.5.1 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 marshmallow-3.26.1 mypy-extensions-1.1.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 ormsgpack-1.10.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse langchain langgraph langchain_openai langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1yglQ464VD-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-a9-bdb6-9f79bfe4aeb7\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-06\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
        "\n",
        "# your openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plm88kELt0zR"
      },
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. get_client() initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKko2X-Wt0zR"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqYMmi6n9Nh1"
      },
      "source": [
        "## Example 1: Simple chat app with LangGraph\n",
        "\n",
        "**What we will do in this section:**\n",
        "\n",
        "*   Build a support chatbot in LangGraph that can answer common questions\n",
        "*   Tracing the chatbot's input and output using Langfuse\n",
        "\n",
        "We will start with a basic chatbot and build a more advanced multi agent setup in the next section, introducing key LangGraph concepts along the way.\n",
        "\n",
        "### Create Agent\n",
        "\n",
        "Start by creating a `StateGraph`. A `StateGraph` object defines our chatbot's structure as a state machine. We will add nodes to represent the LLM and functions the chatbot can call, and edges to specify how the bot transitions between these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGIxgPww6VX6"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
        "\n",
        "# The chatbot node function takes the current State as input and returns an updated messages list. This is the basic pattern for all LangGraph node functions.\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Add a \"chatbot\" node. Nodes represent units of work. They are typically regular python functions.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "# Add an entry point. This tells our graph where to start its work each time we run it.\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Set a finish point. This instructs the graph \"any time this node is run, you can exit.\"\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "\n",
        "# To be able to run our graph, call \"compile()\" on the graph builder. This creates a \"CompiledGraph\" we can use invoke on our state.\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW2SJcRgh7Xo"
      },
      "source": [
        "### Add Langfuse as callback to the invocation\n",
        "\n",
        "Now, we will add then [Langfuse callback handler for LangChain](https://langfuse.com/docs/integrations/langchain/tracing) to trace the steps of our application: `config={\"callbacks\": [langfuse_handler]}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PxEc455-KYM"
      },
      "outputs": [],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "for s in graph.stream({\"messages\": [HumanMessage(content = \"What is Langfuse?\")]},\n",
        "                      config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdf3ZRnWGZ0N"
      },
      "source": [
        "### View traces in Langfuse\n",
        "\n",
        "Example trace in Langfuse: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/85b0c53c4414f22ed8bfc9eb35f917c4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Aq7u6_LBR6"
      },
      "source": [
        "![Trace view of chat app in Langfuse](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_chatapp_trace.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3yyVtGKhMPU"
      },
      "source": [
        "### Visualize the chat app\n",
        "\n",
        "You can visualize the graph using the `get_graph` method along with a \"draw\" method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKkM6mw47kIy"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY0HW5xISntw"
      },
      "source": [
        "```mermaid\n",
        "graph TD;\n",
        "\t__start__([__start__]):::first\n",
        "\tchatbot(chatbot)\n",
        "\t__end__([__end__]):::last\n",
        "\t__start__ --> chatbot;\n",
        "\tchatbot --> __end__;\n",
        "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
        "\tclassDef first fill-opacity:0\n",
        "\tclassDef last fill:#bfb6fc\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1P8hbFNt0zS"
      },
      "source": [
        "### Use Langfuse with LangGraph Server\n",
        "\n",
        "You can add Langfuse as callback when using [LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvAiQIfot0zS"
      },
      "source": [
        "When using the LangGraph Server, the LangGraph Server handles graph invocation automatically. Therefore, you should add the Langfuse callback when declaring the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD7ayEC2t0zS"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
        "\n",
        "def chatbot(state: State):\n",
        "  return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "\n",
        "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# Call \"with_config\" from the compiled graph.\n",
        "# It returns a \"CompiledGraph\", similar to \"compile\", but with callbacks included.\n",
        "# This enables automatic graph tracing without needing to add callbacks manually every time.\n",
        "graph = graph_builder.compile().with_config({\"callbacks\": [langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2W94eY19TR1"
      },
      "source": [
        "## Example 2: Multi agent application with LangGraph\n",
        "\n",
        "**What we will do in this section**:\n",
        "\n",
        "*   Build 2 executing agents: One research agent using the LangChain WikipediaAPIWrapper to search Wikipedia and one that uses a custom tool to get the current time.\n",
        "*   Build an agent supervisor to help delegate the user questions to one of the two agents\n",
        "*   Add Langfuse handler as callback to trace the steps of the supervisor and executing agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WfnrswDdjYTV"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse langgraph langchain langchain_openai langchain_experimental pandas wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tciUQ62IEVec"
      },
      "source": [
        "### Create tools\n",
        "\n",
        "For this example, you build an agent to do wikipedia research, and one agent to tell you the current time. Define the tools they will use below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cet0loyp9p-T"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from datetime import datetime\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# Define a tools that searches Wikipedia\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "# Define a new tool that returns the current datetime\n",
        "datetime_tool = Tool(\n",
        "    name=\"Datetime\",\n",
        "    func = lambda x: datetime.now().isoformat(),\n",
        "    description=\"Returns the current datetime\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31uhDy_mEqr6"
      },
      "source": [
        "### Helper utilities\n",
        "\n",
        "Define a helper function below to simplify adding new agent worker nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75atiExdqd4P"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def create_agent(llm: ChatOpenAI, system_prompt: str, tools: list):\n",
        "    # Each worker node will be given a name and some tools.\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                system_prompt,\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor\n",
        "\n",
        "def agent_node(state, agent, name):\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bZqwU6FCOa"
      },
      "source": [
        "### Create agent supervisor\n",
        "\n",
        "It will use function calling to choose the next worker node OR finish processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu8MzgihrHdF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "members = [\"Researcher\", \"CurrentTime\"]\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    \" following workers:  {members}. Given the following user request,\"\n",
        "    \" respond with the worker to act next. Each worker will perform a\"\n",
        "    \" task and respond with their results and status. When finished,\"\n",
        "    \" respond with FINISH.\"\n",
        ")\n",
        "# Our team supervisor is an LLM node. It just picks the next agent to process and decides when the work is completed\n",
        "options = [\"FINISH\"] + members\n",
        "\n",
        "# Using openai function calling can make output parsing easier for us\n",
        "function_def = {\n",
        "    \"name\": \"route\",\n",
        "    \"description\": \"Select the next role.\",\n",
        "    \"parameters\": {\n",
        "        \"title\": \"routeSchema\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"next\": {\n",
        "                \"title\": \"Next\",\n",
        "                \"anyOf\": [\n",
        "                    {\"enum\": options},\n",
        "                ],\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"next\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Create the prompt using ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Given the conversation above, who should act next?\"\n",
        "            \" Or should we FINISH? Select one of: {options}\",\n",
        "        ),\n",
        "    ]\n",
        ").partial(options=str(options), members=\", \".join(members))\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Construction of the chain for the supervisor agent\n",
        "supervisor_chain = (\n",
        "    prompt\n",
        "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "    | JsonOutputFunctionsParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ognuMaIeFVh7"
      },
      "source": [
        "### Construct graph\n",
        "\n",
        "Now we are ready to start building the graph. Below, define the state and worker nodes using the function we just defined. Then we connect all the edges in the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LwtCmw_rHVz"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import operator\n",
        "from typing import Sequence, TypedDict\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "# The agent state is the input to each node in the graph\n",
        "class AgentState(TypedDict):\n",
        "    # The annotation tells the graph that new messages will always be added to the current states\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    # The 'next' field indicates where to route to next\n",
        "    next: str\n",
        "\n",
        "# Add the research agent using the create_agent helper function\n",
        "research_agent = create_agent(llm, \"You are a web researcher.\", [wikipedia_tool])\n",
        "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
        "\n",
        "# Add the time agent using the create_agent helper function\n",
        "currenttime_agent = create_agent(llm, \"You can tell the current time at\", [datetime_tool])\n",
        "currenttime_node = functools.partial(agent_node, agent=currenttime_agent, name = \"CurrentTime\")\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add a \"chatbot\" node. Nodes represent units of work. They are typically regular python functions.\n",
        "workflow.add_node(\"Researcher\", research_node)\n",
        "workflow.add_node(\"CurrentTime\", currenttime_node)\n",
        "workflow.add_node(\"supervisor\", supervisor_chain)\n",
        "\n",
        "# We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
        "for member in members:\n",
        "    workflow.add_edge(member, \"supervisor\")\n",
        "\n",
        "# Conditional edges usually contain \"if\" statements to route to different nodes depending on the current graph state.\n",
        "# These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n",
        "conditional_map = {k: k for k in members}\n",
        "conditional_map[\"FINISH\"] = END\n",
        "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
        "\n",
        "# Add an entry point. This tells our graph where to start its work each time we run it.\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "\n",
        "# To be able to run our graph, call \"compile()\" on the graph builder. This creates a \"CompiledGraph\" we can use invoke on our state.\n",
        "graph_2 = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3xfJLJyFwBG"
      },
      "source": [
        "### Add Langfuse as callback to the invocation\n",
        "\n",
        "Add [Langfuse handler](https://langfuse.com/docs/integrations/langchain/tracing) as callback: `config={\"callbacks\": [langfuse_handler]}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsX1gw9kryGP"
      },
      "outputs": [],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
        "# You can also set an optional 'run_name' that will be used as the trace name in Langfuse\n",
        "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"How does photosynthesis work?\")]},\n",
        "                      config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqJnMtP5HDql"
      },
      "outputs": [],
      "source": [
        "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
        "for s in graph_2.stream({\"messages\": [HumanMessage(content = \"What time is it?\")]},\n",
        "                      config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4XjtNenH9GF"
      },
      "source": [
        "### See traces in Langfuse\n",
        "\n",
        "Example traces in Langfuse:\n",
        "\n",
        "1. [How does photosynthesis work?](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/7d5f970573b8214d1ca891251e42282c)\n",
        "2. [What time is it?](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/3a69fe4998df50d42054f8944bd6a8d9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5EEBZAIbwc"
      },
      "source": [
        "![Trace view of multi agent in Langfuse](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_multiagent_traces.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCEzabn_jhbf"
      },
      "source": [
        "### Visualize the agent\n",
        "\n",
        "You can visualize the graph using the `get_graph` method along with a \"draw\" method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "notlPjnl-HXV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(graph_2.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESkG2IJS8OY"
      },
      "source": [
        "```mermaid\n",
        "graph TD;\n",
        "\t__start__([__start__]):::first\n",
        "\tResearcher(Researcher)\n",
        "\tCurrentTime(CurrentTime)\n",
        "\tsupervisor(supervisor)\n",
        "\t__end__([__end__]):::last\n",
        "\tCurrentTime --> supervisor;\n",
        "\tResearcher --> supervisor;\n",
        "\t__start__ --> supervisor;\n",
        "\tsupervisor -.-> Researcher;\n",
        "\tsupervisor -.-> CurrentTime;\n",
        "\tsupervisor -. &nbspFINISH&nbsp .-> __end__;\n",
        "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
        "\tclassDef first fill-opacity:0\n",
        "\tclassDef last fill:#bfb6fc\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uybP4h8wGvWw"
      },
      "source": [
        "## Adding scores to traces as scores\n",
        "\n",
        "[Scores](https://langfuse.com/docs/scores/overview) are used to evaluate single observations or entire traces. They enables you to implement custom quality checks at runtime or facilitate human-in-the-loop evaluation processes.\n",
        "\n",
        "In the example below, we demonstrate how to score a specific span for `relevance` (a numeric score) and the overall trace for `feedback` (a categorical score). This helps in systematically assessing and improving your application.\n",
        "\n",
        "**â†’ Learn more about [Custom Scores in Langfuse](https://langfuse.com/docs/scores/custom).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgAqYnQuGwCL"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "# Option 1: Use the yielded span object from the context manager\n",
        "with langfuse.start_as_current_span(\n",
        "    name=\"langgraph-request\") as span:\n",
        "    # ... LangGraph execution ...\n",
        "\n",
        "    # Score using the span object\n",
        "    span.score_trace(\n",
        "        name=\"user-feedback\",\n",
        "        value=1,\n",
        "        data_type=\"NUMERIC\",\n",
        "        comment=\"This was correct, thank you\"\n",
        "    )\n",
        "\n",
        "# Option 2: Use langfuse.score_current_trace() if still in context\n",
        "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
        "    # ... LangGraph execution ...\n",
        "\n",
        "    # Score using current context\n",
        "    langfuse.score_current_trace(\n",
        "        name=\"user-feedback\",\n",
        "        value=1,\n",
        "        data_type=\"NUMERIC\"\n",
        "    )\n",
        "\n",
        "# Option 3: Use create_score() with trace ID (when outside context)\n",
        "langfuse.create_score(\n",
        "    trace_id=predefined_trace_id,\n",
        "    name=\"user-feedback\",\n",
        "    value=1,\n",
        "    data_type=\"NUMERIC\",\n",
        "    comment=\"This was correct, thank you\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq_DeCcXSxwq"
      },
      "source": [
        "### View trace with score in Langfuse\n",
        "\n",
        "Example trace: https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/e60a078b828d4fdc7ea22c73193b0fe4\n",
        "\n",
        "![Trace view including added score](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_score.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cIQVrYZJVMO"
      },
      "source": [
        "## Manage prompts with Langfuse\n",
        "\n",
        "Use [Langfuse prompt management](https://langfuse.com/docs/prompts/example-langchain) to effectively manage and version your prompts. We add the prompt used in this example via the SDK. In production, however, users would update and manage the prompts via the Langfuse UI instead of using the SDK.\n",
        "\n",
        "Langfuse prompt management is basically a Prompt CMS (Content Management System). Alternatively, you can also edit and version the prompt in the Langfuse UI.\n",
        "\n",
        "*   `Name` that identifies the prompt in Langfuse Prompt Management\n",
        "*   Prompt with prompt template incl. `{{input variables}}`\n",
        "*   `labels` to include `production` to immediately use prompt as the default\n",
        "\n",
        "In this example, we create a system prompt for an assistant that translates every user message into Spanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0J8-nbhUUz6"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "langfuse.create_prompt(\n",
        "    name=\"translator_system-prompt\",\n",
        "    prompt=\"You are a translator that translates every input text into Spanish.\",\n",
        "    labels=[\"production\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dullp4XDXhzg"
      },
      "source": [
        "![View prompt in Langfuse UI](https://langfuse.com/images/cookbook/integration-langgraph/integration_langgraph_prompt_example.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNboOjf2YQpD"
      },
      "source": [
        "Use the utility method `.get_langchain_prompt()` to transform the Langfuse prompt into a string that can be used in Langchain.\n",
        "\n",
        "\n",
        "**Context:** Langfuse declares input variables in prompt templates using double brackets (`{{input variable}}`). Langchain uses single brackets for declaring input variables in PromptTemplates (`{input variable}`). The utility method `.get_langchain_prompt()` replaces the double brackets with single brackets. In this example, however, we don't use any variables in our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z49I82blYeXy",
        "outputId": "6cf7cd23-6dde-4e7b-ae50-e369db37c2d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a translator that translates every input text into Spanish.\n"
          ]
        }
      ],
      "source": [
        "# Get current production version of prompt and transform the Langfuse prompt into a string that can be used in Langchain\n",
        "langfuse_system_prompt = langfuse.get_prompt(\"translator_system-prompt\")\n",
        "langchain_system_prompt = langfuse_system_prompt.get_langchain_prompt()\n",
        "\n",
        "print(langchain_system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3zBULfCt0Wq"
      },
      "source": [
        "Now we can use the new system prompt string to update our assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGQhulyMmvZD"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o\", temperature = 0.2)\n",
        "\n",
        "# Add the system prompt for our translator assistent\n",
        "system_prompt = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": langchain_system_prompt\n",
        "}\n",
        "\n",
        "def chatbot(state: State):\n",
        "    messages_with_system_prompt = [system_prompt] + state[\"messages\"]\n",
        "    response = llm.invoke(messages_with_system_prompt)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph_builder.set_finish_point(\"chatbot\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYd7wbttm2ec"
      },
      "outputs": [],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "# Add Langfuse handler as callback: config={\"callbacks\": [langfuse_handler]}\n",
        "for s in graph.stream({\"messages\": [HumanMessage(content = \"What is Langfuse?\")]},\n",
        "                      config={\"callbacks\": [langfuse_handler]}):\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Jrl6jZt0zW"
      },
      "source": [
        "## Add custom spans to a LangGraph trace\n",
        "\n",
        "Sometimes it is helpful to add custom spans to a LangGraph trace. This [GitHub discussion thread](https://github.com/orgs/langfuse/discussions/2988#discussioncomment-11634600) provides an example of how to do this."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}